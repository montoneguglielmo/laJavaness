{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the code I will import the data from the file. Transform the categorical varibale into numerical ones and normalize the value of each variable between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am defining some function to transform process the dataset. The class strToIntGenerator transforms the categorical variables into numberical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "class stringToInt(object):\n",
    "\n",
    "    def __init__(self, datasetDict):\n",
    "\n",
    "        self.datasetDict = datasetDict\n",
    "\n",
    "\n",
    "    def toInt(self, varName, listValue):\n",
    "\n",
    "        isNumber = self.datasetDict[varName]['isNumber']\n",
    "        hasEmpty = self.datasetDict[varName]['hasEmptySymb']\n",
    "        \n",
    "        if isNumber:\n",
    "            listValue = self.toIntFromNumbers(varName, hasEmpty, listValue)\n",
    "\n",
    "        if not isNumber:\n",
    "            listValue = self.toIntFromLabels(varName, listValue)\n",
    "\n",
    "        return listValue\n",
    "\n",
    "    \n",
    "    def toIntFromNumbers(self, varName, hasEmpty, listValue):\n",
    "        \n",
    "        if hasEmpty:\n",
    "           emptySymb = self.datasetDict[varName]['emptySymb']\n",
    "\n",
    "        lstValueFloat = []\n",
    "        for vl in listValue:\n",
    "            if not vl:\n",
    "                lstValueFloat.append(emptySymb)\n",
    "            else:\n",
    "                lstValueFloat.append(float(vl))\n",
    "        return lstValueFloat\n",
    "\n",
    "    \n",
    "        \n",
    "    def toIntFromLabels(self, varName, listValue):\n",
    "        lstValueFloat = []\n",
    "        symb          = datasetDict[varName]['symbols']\n",
    "        for vl in listValue:\n",
    "            lstValueFloat.append(float(symb.index(vl)))\n",
    "        return lstValueFloat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data from the train and test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_data = []\n",
    "with open('test_ljn/train.csv','rb') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter= ' ', quotechar='|')\n",
    "    for row in reader:\n",
    "        row_data.append(row)\n",
    "        \n",
    "row_data_test = []\n",
    "with open('test_ljn/test.csv','rb') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter= ' ', quotechar='|')\n",
    "    for row in reader:\n",
    "        row_data_test.append(row)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a dictionary describing the dataset. The dictionary is called datasetDict. \n",
    "It has an entry for each of the variables X0,...,X14,Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of the datasetDictionary: ['X12', 'X13', 'Y', 'X10', 'X8', 'X9', 'X11', 'X2', 'X3', 'X0', 'X1', 'X6', 'X7', 'X4', 'X5']\n",
      "Keys of one of the dataset entry: ['symbols', 'isNumber', 'n_symbols', 'hasEmptySymb']\n"
     ]
    }
   ],
   "source": [
    "variableName = row_data[0][0].split(';')\n",
    "row_data     = row_data[1:]\n",
    "n_entry      = len(row_data[0][0].split(';'))\n",
    "\n",
    "datasetDict = {}\n",
    "isNumber    = [is_number(rw) for rw in row_data[1][0].split(';')]\n",
    "\n",
    "for cnt in range(n_entry):\n",
    "    variableDict = {'isNumber': isNumber[cnt]}\n",
    "    dataString = []\n",
    "    for ln in row_data:\n",
    "        dataString.append(ln[0].split(';')[cnt])\n",
    "\n",
    "    symbols = list(set(dataString))\n",
    "    variableDict['n_symbols'] = len(symbols)\n",
    "    variableDict['symbols']   = symbols\n",
    "\n",
    "    if symbols[0] == '':\n",
    "        variableDict['hasEmptySymb'] = True\n",
    "        if isNumber[cnt] :\n",
    "            variableDict['emptySymb'] = np.asarray(symbols[1:], dtype=float).mean()\n",
    "    else:\n",
    "        variableDict['hasEmptySymb'] = False\n",
    "\n",
    "\n",
    "    datasetDict[variableName[cnt]] =  variableDict\n",
    "\n",
    "print \"Keys of the datasetDictionary:\", datasetDict.keys()\n",
    "print \"Keys of one of the dataset entry:\", datasetDict['X1'].keys()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class strToIntGenerator transforms the categorical variables into numberical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "strToIntGenerator = stringToInt(datasetDict)  \n",
    "\n",
    "\n",
    "train_set = []\n",
    "for cnt in range(n_entry):\n",
    "        varName  = variableName[cnt]\n",
    "        data_col = [] \n",
    "        for ln in row_data:\n",
    "            data_col.append(ln[0].split(';')[cnt])\n",
    "        data_col = strToIntGenerator.toInt(varName, data_col)\n",
    "        train_set.append(data_col)\n",
    "\n",
    "row_data_test   = row_data_test[1:]\n",
    "test_set        = []\n",
    "for cnt in range(n_entry-1):\n",
    "        varName  = variableName[cnt]\n",
    "        data_col = [] \n",
    "        for ln in row_data_test:\n",
    "            data_col.append(ln[0].split(';')[cnt])\n",
    "        data_col = strToIntGenerator.toInt(varName, data_col)\n",
    "        test_set.append(data_col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of the variables are normalized between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_norm = []\n",
    "test_set_norm  = []\n",
    "for tr, ts in zip(train_set[:-1], test_set):\n",
    "    tr = np.asarray(tr)\n",
    "    tr = (tr - tr.min())/(tr.max() - tr.min())\n",
    "    ts = (ts - tr.min())/(tr.max() - tr.min())\n",
    "    train_set_norm.append(tr)\n",
    "    test_set_norm.append(ts)\n",
    "\n",
    "train_set_norm.append(np.asarray(train_set[-1]))\n",
    "\n",
    "train_set_norm = np.hstack([tr.reshape(-1,1) for tr in train_set_norm])\n",
    "test_set_norm  = np.hstack([tr.reshape(-1,1) for tr in test_set_norm])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes are not balanced, the number of data belonging to the class 0 are much more then the ones\n",
    "belonging to class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data belonging to class 1: 7841\n",
      "Total number of data: 32561\n"
     ]
    }
   ],
   "source": [
    "print 'Number of data belonging to class 1:', int(train_set_norm[:,-1].sum())\n",
    "print 'Total number of data:', train_set_norm.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have dataset with balanced classes I decided to prepare three different datasets.\n",
    "Each of the three datasets will have all the data belonging to class 1 and 1/3 of the data beloning to class 0.\n",
    "In this way each dataset will be balanced. I will then train three different classifiers on the three datasets and avarege their outputs to obtain the predicted class for the test set. Each dataset will be composed of a train and a validation set. The validation set was set to be 1/10 of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_one  = np.where(train_set_norm[:,-1]>.5)[0]\n",
    "idx_zero = np.where(train_set_norm[:,-1]<.5)[0]\n",
    "np.random.shuffle(idx_zero)\n",
    "\n",
    "trainGroup = []\n",
    "n_groups   = 3\n",
    "sz_group   = idx_zero.shape[0]/n_groups\n",
    "for cnt in range(n_groups):\n",
    "    idx_zero_group = idx_zero[cnt*sz_group:(cnt+1)*sz_group]\n",
    "    trainGroup.append(np.vstack([train_set_norm[idx_one,:], train_set_norm[idx_zero_group,:]]))\n",
    "\n",
    "\n",
    "trainValidGroup = []\n",
    "valid_frac      = 0.1\n",
    "for gr in trainGroup:\n",
    "    idx   = np.arange(gr.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    n_valid = int(gr.shape[0] * valid_frac)\n",
    "    valid   = gr[idx[:n_valid]]\n",
    "    train   = gr[idx[n_valid:]]\n",
    "\n",
    "    trainValidGroup.append([valid,train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy\n",
    "import numpy as np\n",
    "import numpy.random as rng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a model class: in the following a will build two kinds of model, a logistic regrassion and a neural network, both classes will inherit from the class classifier that here defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier(object):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def negative_log_likelihood(self, y):\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "\n",
    "\n",
    "    def get_updates(self, cost, learning_rate):\n",
    "       grds     = [T.grad(cost=cost, wrt=param) for param in self.params]\n",
    "       updates  = [(param, param - learning_rate * grd) for param, grd in zip(self.params, grds)]\n",
    "       return updates\n",
    "       \n",
    "    def errors(self, y):\n",
    "        return T.mean(T.neq(self.y_pred, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the class Logistic regrassion that inherit from the class classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(classifier):\n",
    "\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        self.W = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_in, n_out),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "        \n",
    "        self.b = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_out,),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='b',\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
    "        self.params = [self.W, self.b]\n",
    "        self.input = input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the class NN that also inherit from the class classifier.\n",
    "The following class implements a FFW neural network.\n",
    "n_in: dimension of the input\n",
    "n_out: dimension of the output\n",
    "n_hidden: number of node in the hidden layers\n",
    "n_layers: number of hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(classifier):\n",
    "\n",
    "    def __init__(self, input, n_in, n_out, n_hidden, n_layers):\n",
    "\n",
    "        \n",
    "        W_values = [np.asarray(rng.uniform(low=-numpy.sqrt(6. / (n_in + n_hidden)),\n",
    "                                           high=numpy.sqrt(6. / (n_in + n_hidden)),\n",
    "                                           size=(n_in, n_hidden)\n",
    "                              ), dtype=theano.config.floatX)]\n",
    "\n",
    "        b_values = [numpy.zeros((n_hidden,), dtype=theano.config.floatX)]\n",
    "\n",
    "        \n",
    "        for _ in range(n_layers):\n",
    "            W_values.append(np.asarray(rng.uniform(low   = -numpy.sqrt(6. / (n_hidden + n_hidden)),\n",
    "                                                   high  = numpy.sqrt(6. / (n_hidden + n_hidden)),\n",
    "                                                   size  = (n_hidden, n_hidden)),\n",
    "                                                   dtype = theano.config.floatX))\n",
    "\n",
    "            b_values.append(numpy.zeros((n_hidden,), dtype=theano.config.floatX))\n",
    "\n",
    "\n",
    "            \n",
    "        W_values.append(np.asarray(rng.uniform(low  = -numpy.sqrt(6. / (n_hidden + n_out)),\n",
    "                                                   high = numpy.sqrt(6. / (n_hidden + n_out)),\n",
    "                                                   size = (n_hidden, n_out)),\n",
    "                                                   dtype= theano.config.floatX))\n",
    "\n",
    "        b_values.append(numpy.zeros((n_out,), dtype=theano.config.floatX))\n",
    "        \n",
    "\n",
    "        self.Ws = [theano.shared(value=W, borrow=True) for W in W_values]\n",
    "        self.bs = [theano.shared(value=b, borrow=True) for b in b_values]\n",
    "\n",
    "        self.p_y_given_x = T.nnet.relu(T.dot(input, self.Ws[0]) + self.bs[0])\n",
    "        \n",
    "        for cnt_l in range(n_layers):\n",
    "            self.p_y_given_x = T.nnet.relu(T.dot(self.p_y_given_x, self.Ws[cnt_l+1]) + self.bs[cnt_l+1])  \n",
    "\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(self.p_y_given_x, self.Ws[-1]) + self.bs[-1])\n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
    "        self.params = self.Ws + self.bs\n",
    "        self.input = input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cast the datasets into theano shared variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = trainValidGroup#np.load('dataTrain.npy')\n",
    "\n",
    "trainx = []\n",
    "trainy = []\n",
    "validx = []\n",
    "validy = []\n",
    "for gr in data:\n",
    "    validx.append(theano.shared(np.asarray(gr[0][:,:-1], dtype=theano.config.floatX)))\n",
    "    validy.append(T.cast(theano.shared(np.asarray(gr[0][:,-1], dtype=int)), 'int32'))\n",
    "    trainx.append(theano.shared(np.asarray(gr[1][:,:-1], dtype=theano.config.floatX)))\n",
    "    trainy.append(T.cast(theano.shared(np.asarray(gr[1][:,-1], dtype=int)), 'int32'))\n",
    "       \n",
    "\n",
    "valid_sets_x = validx\n",
    "valid_sets_y = validy\n",
    "train_sets_x = trainx\n",
    "train_sets_y = trainy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size    = 30\n",
    "n_epochs      = 100\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling theano function for training and validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = T.lscalar()\n",
    "    \n",
    "x = T.matrix('x') \n",
    "y = T.ivector('y')\n",
    "\n",
    "n_models    = 3\n",
    "#classifiers = [LogisticRegression(input=x, n_in=14, n_out=2) for cnt in range(n_models)]\n",
    "classifiers = [NN(input=x, n_in=14, n_out=2, n_hidden=50, n_layers=1) for cnt in range(n_models)]\n",
    "    \n",
    "    # the cost we minimize during training is the negative log likelihood of\n",
    "    # the model in symbolic format\n",
    "costs = [classifier.negative_log_likelihood(y) for classifier in classifiers]\n",
    "\n",
    "validate_models = [\n",
    "    theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=classifier.errors(y),\n",
    "        givens={ x: valid_set_x[index * batch_size: (index + 1) * batch_size], \n",
    "                 y: valid_set_y[index * batch_size: (index + 1) * batch_size]\\\n",
    "               }\n",
    "    )for classifier, valid_set_x, valid_set_y in zip(classifiers, valid_sets_x, valid_sets_y)]\n",
    "\n",
    "updates = [classifier.get_updates(cost, learning_rate) for classifier, cost in zip(classifiers, costs)]\n",
    "\n",
    "train_models = [\n",
    "    theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        updates=update,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    ) for cost, update, train_set_x, train_set_y in zip(costs, updates, train_sets_x, train_sets_y)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    \n",
    "epoch = 0\n",
    "while (epoch < n_epochs):\n",
    "    epoch = epoch + 1\n",
    "\n",
    "    train_error = [0.]*n_models\n",
    "    cnt_btc     = 0\n",
    "    for minibatch_index in range(n_train_batches):\n",
    "        for cnt_mdl in range(n_models):\n",
    "            train_error[cnt_mdl] += train_models[cnt_mdl](minibatch_index)\n",
    "        cnt_btc     += 1\n",
    "            \n",
    "    train_error = np.asarray(train_error)/float(cnt_btc)\n",
    "\n",
    "    cnt_btc_valid = 0\n",
    "    validation_losses = [0.] * n_models\n",
    "    for minibatch_index in range(n_valid_batches):\n",
    "        for cnt_mdl in range(n_models):\n",
    "            validation_losses[cnt_mdl]  += validate_models[cnt_mdl](minibatch_index)\n",
    "        cnt_btc_valid += 1\n",
    "\n",
    "    validation_losses = np.asarray(validation_losses)/float(cnt_btc_valid)\n",
    "\n",
    "    if np.mod(epoch, 10)==0:\n",
    "        print('epoch %i' %epoch)\n",
    "        for cnt_mdl in range(n_models):\n",
    "            print('Model %i: train error %.4f, validation error %.4f' %(cnt_mdl, train_error[cnt_mdl], validation_losses[cnt_mdl] * 100.))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
